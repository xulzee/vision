# OpenPAI

参考 [microsoft/pai]( https://github.com/Microsoft/pai )

## 目录

1. [适用场景](#适用场景)
2. [特点](#特点)
3. [入门](#入门)
4. [部署](#部署)

## 适用场景

1. 在团队间共享强大的 AI 计算资源（例如，GPU、FPGA 集群）。
2. 在组织内共享或重用 AI 资产（如模型、数据、运行环境等) 。
3. 构建易于 IT 运维管理的 AI 计算平台。
4. 在同一个环境中完成模型训练过程。

## 特点

OpenPAI 的设计成熟可靠。在微软的大规模部署中，得到了多年持续运行的验证。

### 易于部署

OpenPAI 是全栈的解决方案。 不仅支持本地、公有云及混合云中的部署，还支持单机试用的部署。

### 支持流行的 AI 框架以及异构的硬件

OpenPAI 提供了预构建的支持主流 AI 框架的 Docker。 支持添加异构硬件。 支持分布式训练, 如分布式 TensorFlow。

### 全栈解决方案、易于扩展

OpenPAI 是支持深度学习、虚拟集群，兼容 Hadoop/Kubernetes 生态系统的完整解决方案。 OpenPAI 支持可扩展组件：可根据需要接入扩展模块。

## 入门

OpenPAI 用于管理计算资源，并对机器学习任务进行了优化。 通过 Docker 技术，硬件计算资源与软件相分离。这样，用户能轻松的进行分布式计算，在不同的深度学习框架间切换，也能在完全一致的环境中重复运行 Job。

作为平台，OpenPAI 需要[部署](#部署)后才能使用。 OpenPAI 也支持单机部署。

部署完成后，可参考[训练模型](#训练模型)。

## 部署

根据以下内容来检查先决条件，部署并验证 OpenPAI 集群。 初次部署完成后，还可以根据需要添加新的服务器。

强烈建议在空闲的服务器上安装 OpenPAI。 有关硬件规范，参考[这里](https://github.com/Microsoft/pai/wiki/Resource-Requirement)。

### 先决条件和准备工作

* Ubuntu 16.04 。

* 每台服务器都有**静态** IP 地址，并确保服务器可以相互通信。

* 确保服务器可以访问互联网，特别是 Docker Hub 或其镜像服务器。 在部署过程中需要拉取 OpenPAI 的 Docker 映像。

* 确保 SSH 服务已启用，所有服务器使用相同的用户名、密码，并启用 sudo 权限。

* 确保 NTP 服务已启用。

* 建议不提前安装 Docker 组件，如果已安装，确保 Docker 版本高于 1.26。

* OpenPAI 会保留部分内存和 CPU 资源来运行服务，须确保服务器有足够的资源来运行机器学习作业。 详情参考[硬件要求](https://github.com/Microsoft/pai/wiki/Resource-Requirement)。

* OpenPAI 的服务器不能提供其它服务。 OpenPAI 会管理服务器的所有 CPU、内存和 GPU 资源。 如果服务器上有其它的服务负载，可能导致资源不足而产生各种问题。

* **安装显卡驱动（安装完成之后需要重启一次机器！！）**

  ```bash
  # add driver source
  sudo add-apt-repository ppa:graphics-drivers/ppa
  sudo apt-get update
  
  # remove Old driver
  sudo apt-get purge nvidia*
  
  # install new driver (The same settings are used here as later ~ / pai-config / services-configuration.yaml)
  sudo apt install nvidia-418
  ```

* 安装docker

  ```bash
  # install docker
  sudo apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common
  curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
  sudo add-apt-repository \
     "deb [arch=amd64] https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/ubuntu \
     $(lsb_release -cs) \
     stable"
     
  sudo apt-get update
  sudo apt-get install docker-ce
  
  # change docker hub source
  sudo mkdir -p /etc/docker
  sudo tee /etc/docker/daemon.json <<-'EOF'
  {
    "registry-mirrors": ["https://4h6bke89.mirror.aliyuncs.com"]
  }
  EOF
  sudo systemctl daemon-reload
  sudo systemctl restart docker
  ```

  

* **安装nvidia-docker**

  ```bash
  # If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containers
  docker volume ls -q -f driver=nvidia-docker | xargs -r -I{} -n1 docker ps -q -a -f volume={} | xargs -r docker rm -f
  sudo apt-get purge -y nvidia-docker
  
  # Add the package repositories
  curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \
    sudo apt-key add -
  distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
  curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
    sudo tee /etc/apt/sources.list.d/nvidia-docker.list
  sudo apt-get update
  
  # Install nvidia-docker2 and reload the Docker daemon configuration
  sudo apt-get install -y nvidia-docker2
  sudo pkill -SIGHUP dockerd
  
  # Test nvidia-smi with the latest official CUDA image
  docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi
  ```

  



### 部署

对于小于 50 台服务器的中小型集群，参考[使用默认设置部署](#使用默认配置部署)，用最简单的方式来部署 OpoenPAI。 在默认配置的基础上，可针对不同的硬件环境和使用场景来定制优化部署方案。

#### 使用默认配置部署

对于小于 50 台服务器的中小型集群, 建议[使用默认配置部署](#使用默认配置部署小于50台服务器的中小型集群)。。

对于大型集群，仍需要根据此向导来生成默认配置，然后再[自定义部署配置](#自定义部署)。



#### 使用默认配置部署小于50台服务器的中小型集群

- Step 1. Prepare Deployment Environment
- Step 2. Prepare Configuration
- Step 3. Deploy Kubernetes
- Step 4. Update cluster configuration into Kubernetes
- Step 5. Start all OpenPAI services

* * *

### Step 1. Prepare Deployment Environment 

#### Table of Contents

- 1. What's dev-box?
- 2. Install docker on your server
     - 2.1 OptionA: Use prebuild dev-box image
     - 2.2 OptionB: build dev-box docker image on your own
- 3. Deploy dev-box over existing K8s

##### What's dev-box? 

Dev-Box is a docker container which contains necessary dependent software for paictl to deploy and manage you cluster. With a dev-box, you no longer need to install the software in your host environment, make your host environment's software package clean.

##### Install docker on your server

    dev-box is a docker container used to boot up or/and maintain a PAI cluster. For convenience, we provide a prebuild Docker image on Docker Hub.

###### OptionA: Use prebuild dev-box image 

Notice that `dev-box` should run on a machine outside of PAI cluster, it shouldn't run on any PAI cluster node. **replace below v0.x.y to latest release, which can be found [here](https://github.com/Microsoft/pai/releases). For example: v0.14.0**

```bash
# Pull the dev-box image from Docker Hub
sudo docker pull docker.io/openpai/dev-box:v0.x.y

# Run your dev-box
# Assume the path of custom-hadoop-binary-path in your service-configuration is /pathHadoop,
#   and the path of your cluster-configuration is /pathConfiguration.
# By now, you can leave it as it is, we only mount those two directories into docker container for later usage.
sudo docker run -itd \
        -e COLUMNS=$COLUMNS -e LINES=$LINES -e TERM=$TERM \
        -v /var/run/docker.sock:/var/run/docker.sock \
        -v /pathConfiguration:/cluster-configuration  \
        -v /hadoop-binary:/hadoop-binary  \
        --pid=host \
        --privileged=true \
        --net=host \
        --name=dev-box \
        docker.io/openpai/dev-box:v0.x.y

# Working in your dev-box
sudo docker exec -it dev-box /bin/bash
cd /pai

# Now you are free to configure your cluster and run PAI commands...

```

###### OptionB: build dev-box docker image on your own

1. Build dev-box on the latest code

**Notice**, replace v0.x.y as above, if you are trying to deploy OpenPAI. You can also remove -b parameter, if you are contributing to latest OpenPAI, but it's unstable.

```bash
<br /># if you are trying to install latest release, replace v0.x.y like to v0.9.5. If you are trying to contribute on OpenPAI, you can remove -b parameter and clone to default branch.
git clone -b v0.x.y http://yun.nju.edu.cn:8081/xulzee/pai.git

# Go into the workdir.
cd pai/src/dev-box/

# change locale dockerfile
wget http://yun.nju.edu.cn/f/7d7dad554b/?raw=1
mv index.html?raw=1 build/dev-box.dockerfile

# Build your dev-box.
sudo docker build -t dev-box . --file=build/dev-box.dockerfile

```

2. Start your dev-box container

- Suppose the directory path of your cluster-configuration is `/pathConfiguration`. Note: Don't change the configuration file name！

```bash
# Run your dev-box
sudo docker run -itd \
        -e COLUMNS=$COLUMNS -e LINES=$LINES -e TERM=$TERM \
        -v /var/run/docker.sock:/var/run/docker.sock \
        -v /pathConfiguration:/cluster-configuration  \
        -v /hadoop-binary:/hadoop-binary  \
        --pid=host \
        --privileged=true \
        --net=host \
        --name=dev-box \
        dev-box

# Working in your dev-box
sudo docker exec -it dev-box /bin/bash
cd /pai
git checkout v0.x.y

# Now you are free to configure your cluster and run PAI commands...

```

##### Deploy dev-box over existing K8s 

If you want to deploy dev-box in **already** deployed kubernetes.

Prerequisites: The user has installed kubectl on the current machine.

(1) Create a label for the server to be deployed:

```bash
kubectl label --overwrite=true nodes $NODE-IP-ADDRESS dev-box=true
```

(2) Deploy dev-box to kubernetes

```bash
cd pai/src/dev-box

kubectl create -f dev-box-k8s-deploy.yaml
```

* * *

### Step 2. Prepare Configuration 

1. #### Write Quick Start Configuration

```YAML
# quick-start.yaml

# (Required) Please fill in the IP address of the server you would like to deploy OpenPAI
machines:

  - 192.168.1.11
  - 192.168.1.12
  - 192.168.1.13

# (Required) Log-in info of all machines. System administrator should guarantee
# that the username/password pair or username/key-filename is valid and has sudo privilege.
ssh-username: pai
ssh-password: pai-password

# (Optional, default=None) the key file that ssh client uses, that has higher priority then password.
#ssh-keyfile-path: <keyfile-path>

# (Optional, default=22) Port number of ssh service on each machine.
#ssh-port: 22

# (Optional, default=DNS of the first machine) Cluster DNS.
#dns: <ip-of-dns>

# (Optional, default=10.254.0.0/16) IP range used by Kubernetes. Note that
# this IP range should NOT conflict with the current network.
#service-cluster-ip-range: <ip-range-for-k8s>
```

2. #### Generate OpenPAI configuration files

##### (1) generate configuration files

```bash
cd /pai

# cmd should be executed under pai directory in the dev-box.

python paictl.py config generate -i /pai/deployment/quick-start/quick-start.yaml -o ~/pai-config -f
```

##### (2) update docker tag to release version

```bash
vi ~/pai-config/services-configuration.yaml
```

For example: v0.x.y branch, user should change docker-tag to v0.x.y.

```bash
docker-tag: v0.x.y
data-path: /datastorage
cluster-id: vision
```

##### (3) changing gpu count and type

Quick start will generate node with 1 gpu with type generic, this may not suit your situation, for example, if you have two types of machines, and one type has 4 Tesla K80 gpu cards, and another has 2 Tesla P100 cards, you should modify your ~/pai-config/layout.yaml as following:

```YAML
machine-sku:
  k80-node:
    mem: 40G
    gpu:
      type: Tesla K80
      count: 4
    cpu:
      vcore: 24
    os: ubuntu16.04
  p100-node:
    mem: 20G
    gpu:
      type: Tesla P100
      count: 2
    cpu:
      vcore: 24
    os: ubuntu16.04

machine-list:

  - hostname: xxx
    hostip: yyy
    machine-type: k80-node
  - hostname: xxx
    hostip: yyy
    machine-type: p100-node
```

3. #### Customize configure OpenPAI

* * *

### Step 3. Deploy Kubernetes 

If your cluster is deployed in Azure, and there are azure rdma capable machines. Please go to this [section](#az_rdma) first.

#### Prerequires

将 `~/pai-config/kubernetes-configuration.yaml`

```
# The docker registry used in the k8s deployment. If you can access to gcr, we suggest to use gcr.

docker-registry: gcr.io/google_containers 
```

改为 change to :

```
docker-registry: docker.io/mirrorgooglecontainers
```

#### Command

```bash
cd pai

python paictl.py cluster k8s-bootup -p ~/pai-config
```

The `paictl` tool does the following things:

- Install `kubectl` command in the current machine (or the dev-box).
- Generate Kubernetes-related configuration files based on `layout.yaml`, `kubernetes-configuration.yaml` and `k8s-role-definition.yaml`.
- Use `kubectl` to boot up Kubernetes on target machines.

#### How to check 

After this step, the system maintainer can check the status of Kubernetes by accessing Kubernetes Dashboard:

    http://<master>:9090


Where `<master>` denotes the IP address of the load balancer of Kubernetes master nodes. When there is only one master node and a load balancer is not used, it is usually the IP address of the master node itself.

* * *

### Step 4. Update cluster configuration into Kubernetes 

#### Update Configuration

After the kubernetes cluster is setup, and before managing your cluster and service, you should upload the cluster configuration into the kubernetes cluster with the following command.

```bash
python paictl.py config push -p /path/to/config/dir [-c ~/.kube/config]
```

* * *

### Step 5. Start all OpenPAI services 

When Kubernetes is up and running, PAI services can then be deployed to it using `paictl` tool:

```bash
cd pai

# cmd should be executed under /pai directory in the environment.

python paictl.py service start [ -c ~/.kube/config] [ -n service-list ]
```

If the `-n` parameter is specified, only the given services, e.g. `rest-server`, `webportal`, `watchdog`, etc., will be deployed. If not, all PAI services will be deployed. In the latter case, the above command does the following things:

- Generate Kubernetes-related configuration files based on `layout.yaml`.

- Use `kubectl` to set up config maps and create pods on Kubernetes.

### How to check

After this step, the system maintainer can check the status of OpenPAI services by accessing OpenPAI kubernetes web portal:

```bash
http://<master>:9090/#!/pod?namespace=default
```

* * *

### Appendix. Validate deployment 

#### 1 Check Drivers 

#### 1.1 Check Drivers service's log 

Dashboard:

    http://<master>:9090


search driver, view driver status

![PAI_search_driver](asset/PAI_search_driver.png)

view driver logs, this log shows driver in health status

![PAI_driver_right](asset/PAI_driver_right.png)

#### 1.2 Check Drivers version 

```bash
# (1) find driver container at server
~$ sudo docker ps | grep driver

daeaa9a81d3f        aiplatform/drivers                                    "/bin/sh -c ./inst..."   8 days ago          Up 8 days                                    k8s_nvidia-drivers_drivers-one-shot-d7fr4_default_9d91059c-9078-11e8-8aea-000d3ab5296b_0
ccf53c260f6f        gcr.io/google_containers/pause-amd64:3.0              "/pause"                 8 days ago          Up 8 days                                    k8s_POD_drivers-one-shot-d7fr4_default_9d91059c-9078-11e8-8aea-000d3ab5296b_0

# (2) login driver container

~$ sudo docker exec -it daeaa9a81d3f /bin/bash

# (3) checker driver version

root@~/drivers# nvidia-smi
Fri Aug  3 01:53:04 2018
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.111                Driver Version: 384.111                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K80           On   | 0000460D:00:00.0 Off |                    0 |
| N/A   31C    P8    31W / 149W |      0MiB / 11439MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

```

### 2 Data path check 

A configuration in ```service-configuration.yaml```'s ```cluster.commmon.data-path```. The default value is ```/datastorage```

```bash
<br />#SSH to the master machine

~$ ls /datastorage

hadooptmp  hdfs  launcherlogs  prometheus  yarn  zoodata

```

### 3 Admin Account in Webportal 

Dashboard:

    http://<master>:9286/virtual-clusters.html


try to login:

![PAI_login](asset/PAI_login.png)

Note: The username and password are configured in the ```service-configuraiton.yaml```'s ```rest-server``` field.

### 4 Troubleshooting OpenPAI services 

#### 4.1 Diagnosing the problem 

- Monitor

  From kubernetes webportal:

Dashboard:

    http://<master>:9090


![PAI_deploy_log](asset/PAI_deploy_pod.png)

    From OpenPAI watchdog:

[OpenPAI watchdog](../../alerting/watchdog-metrics.md)

- Log

  From kubernetes webportal:

![PAI_deploy_pod](asset/PAI_deploy_log.png)

    From each node container / pods log file:

View containers log under folder:

```bash
ls /var/log/containers
```

View pods log under folder:

```bash
ls /var/log/pods
```

- Debug

As OpenPAI services are deployed on kubernetes, please refer [debug kubernetes pods](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/)

#### 4.2 Fix problem 

- Update OpenPAI Configuration

Check and refine 4 yaml files:

        - layout.yaml
        - kubernetes-configuration.yaml
        - k8s-role-definition.yaml
        - serivices-configuration.yaml


- Customize config for specific service

If user want to customize single service, you could find service config file at [src](../../../../src) and find image dockerfile at [src](../../../../src).

- Update Code & Image

  - Customize image dockerfile or code

User could find service's image dockerfile at [src](../../../../src) and customize them.

- Rebuild image

User could execute the following cmds:

Build docker image

```bash
    paictl.py image build -p /path/to/configuration/ [ -n image-x ]
```

Push docker image

```bash
    paictl.py image push -p /path/to/configuration/ [ -n image-x ]
```

If the `-n` parameter is specified, only the given image, e.g. `rest-server`, `webportal`, `watchdog`, etc., will be build / push.

#### 4.3 Reboot service 

1.     Stop single or all services.

```bash
python paictl.py service stop \
  [ -c /path/to/kubeconfig ] \
  [ -n service-list ]
```

If the -n parameter is specified, only the given services, e.g. rest-server, webportal, watchdog, etc., will be stopped. If not, all PAI services will be stopped.

2.     Boot up single all OpenPAI services.

Please refer to this [section](./distributed-deploy.md#c-step-5) for details.

### 5 Troubleshooting Kubernetes Clusters 

Please refer [Kubernetes Troubleshoot Clusters](https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/)

### 6 Getting help 

- StackOverflow: If you have questions about OpenPAI, please submit question at Stackoverflow under tag: openpai
- [Report an issue:](https://github.com/Microsoft/pai/wiki/Issue-tracking) If you have issue/ bug/ new feature, please submit it at Github



### 验证部署

可提交 hello-world Job 进行端到端的验证。